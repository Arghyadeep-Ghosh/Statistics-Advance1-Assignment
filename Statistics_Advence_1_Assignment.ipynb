{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv_WdVSok0lj"
      },
      "outputs": [],
      "source": [
        "# Q1)  Explain the properties of the F-distribution.\n",
        "\n",
        "# ANS)\n",
        "# Properties of the F-distribution\n",
        "\n",
        "# 1. Non-negative Values\n",
        "#    - The F-distribution only takes non-negative values, starting from zero\n",
        "#      and extending to infinity. It is always positive because it is based\n",
        "#      on the ratio of variances, which cannot be negative.\n",
        "\n",
        "# 2. Right-skewed Distribution\n",
        "#    - The F-distribution is right-skewed, with most probability mass on the left\n",
        "#      and a long tail extending to the right.\n",
        "#    - As the degrees of freedom increase, the skew decreases, making the F-distribution\n",
        "#      approach a normal shape.\n",
        "\n",
        "# 3. Dependent on Degrees of Freedom\n",
        "#    - The shape of the F-distribution is determined by two sets of degrees of freedom:\n",
        "#      d1 (numerator) and d2 (denominator).\n",
        "#    - Higher values for either parameter make the distribution more symmetric\n",
        "#      and closer to a normal distribution.\n",
        "\n",
        "# 4. Asymmetry\n",
        "#    - Unlike the normal distribution, the F-distribution is asymmetric. Its shape\n",
        "#      changes based on its degrees of freedom, becoming more bell-shaped as degrees\n",
        "#      of freedom increase.\n",
        "\n",
        "# 5. Mean and Variance\n",
        "#    - The mean of the F-distribution exists only when d2 > 2 and is given by:\n",
        "#      Mean = d2 / (d2 - 2)\n",
        "#    - The variance exists only when d2 > 4 and is given by:\n",
        "#      Variance = [2 * d2^2 * (d1 + d2 - 2)] / [d1 * (d2 - 2)^2 * (d2 - 4)]\n",
        "\n",
        "# 6. Applications in Hypothesis Testing\n",
        "#    - The F-distribution is mainly used in hypothesis testing to compare variances.\n",
        "#    - For example, in ANOVA, it helps determine if there is a significant difference\n",
        "#      between the means of multiple groups by examining the ratio of variance\n",
        "#      between groups to variance within groups.\n",
        "#    - It is also used in regression analysis to test the overall significance of a model.\n",
        "\n",
        "# 7. Relationship to Other Distributions\n",
        "#    - If two independent chi-square distributed random variables are divided by their\n",
        "#      respective degrees of freedom, the ratio follows an F-distribution.\n",
        "#    - Specifically, if X ~ Chi-Square(d1) and Y ~ Chi-Square(d2), then:\n",
        "#      F = (X/d1) / (Y/d2) follows an F(d1, d2) distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2)  In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "\n",
        "# Ans)\n",
        "# The F-distribution is commonly used in several types of statistical tests,\n",
        "# especially those involving variances. Here are some of the primary tests\n",
        "# that use the F-distribution and the reasons why it is appropriate:\n",
        "\n",
        "# 1. Analysis of Variance (ANOVA)\n",
        "#    - ANOVA uses the F-distribution to test if there is a significant difference\n",
        "#      between the means of multiple groups.\n",
        "#    - The F-statistic in ANOVA compares the variance between groups to the variance\n",
        "#      within groups. If the ratio (F-value) is high, it suggests that at least\n",
        "#      one group mean is significantly different.\n",
        "#    - The F-distribution is appropriate because it allows testing multiple group\n",
        "#      variances simultaneously and is designed for ratios of variances.\n",
        "\n",
        "# 2. Regression Analysis\n",
        "#    - In regression analysis, the F-distribution tests the overall significance\n",
        "#      of a model, checking if the independent variables together explain a\n",
        "#      significant amount of variance in the dependent variable.\n",
        "#    - The F-test compares the variance explained by the model to the variance\n",
        "#      unexplained (error), providing a measure of the modelâ€™s effectiveness.\n",
        "#    - This test is suitable because the F-distribution handles the ratio of\n",
        "#      two variances, which aligns with comparing explained and unexplained variances.\n",
        "\n",
        "# 3. Testing Equality of Two Variances\n",
        "#    - The F-distribution is also used to compare the variances of two independent\n",
        "#      samples to determine if they are significantly different.\n",
        "#    - The test calculates the ratio of the variances of two samples; if this ratio\n",
        "#      is significantly high or low, it suggests unequal variances.\n",
        "#    - The F-distribution is ideal here as it directly measures the ratio of two\n",
        "#      sample variances, which aligns with testing for variance equality.\n",
        "\n",
        "# Why the F-distribution is Appropriate:\n",
        "#    - The F-distribution is designed for scenarios where we analyze ratios of variances.\n",
        "#    - Since variances are always non-negative, the F-distribution (also non-negative\n",
        "#      and right-skewed) is well-suited for representing these ratios.\n",
        "#    - Its sensitivity to degrees of freedom allows flexibility in representing variance\n",
        "#      ratios from different sample sizes.\n"
      ],
      "metadata": {
        "id": "7UWeV7UgmTFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3) What are the key assumptions required for conducting an F-test to compare the variances of two populations?\n",
        "\n",
        "# Ans)\n",
        "# Key Assumptions for Conducting an F-test to Compare Variances:\n",
        "\n",
        "# 1. Independence of Observations\n",
        "#    - The observations within each sample and between the two samples must be independent.\n",
        "#    - This means that the value of one observation should not influence or be related\n",
        "#      to the value of another.\n",
        "\n",
        "# 2. Normality of Populations\n",
        "#    - The populations from which the samples are drawn should be normally distributed.\n",
        "#    - The F-test is sensitive to deviations from normality, so if this assumption is\n",
        "#      violated, the results may not be reliable.\n",
        "\n",
        "# 3. Random Sampling\n",
        "#    - Each sample should be randomly selected from the population.\n",
        "#    - Random sampling helps ensure that the samples are representative of the populations\n",
        "#      being compared.\n",
        "\n",
        "# 4. Ratio of Variances\n",
        "#    - The F-test compares the ratio of variances. The test assumes that if the null\n",
        "#      hypothesis is true, the ratio of population variances is 1.\n",
        "#    - A significant result indicates a deviation from this ratio, suggesting unequal variances.\n",
        "\n",
        "# Note:\n",
        "#    - If the assumptions of normality or independence are not met, other tests\n",
        "#      (such as the Levene's test or Brown-Forsythe test) may be more appropriate\n",
        "#      for comparing variances.\n"
      ],
      "metadata": {
        "id": "yeLc2p1Lmo1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4) What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "\n",
        "#Ans)\n",
        "# Purpose of ANOVA (Analysis of Variance):\n",
        "#    - ANOVA is used to determine if there are statistically significant differences\n",
        "#      between the means of three or more groups.\n",
        "#    - It helps identify whether at least one group mean is different from others\n",
        "#      without needing to conduct multiple t-tests, which could increase the risk\n",
        "#      of Type I errors.\n",
        "#    - By comparing variances within and between groups, ANOVA reveals if any observed\n",
        "#      differences in group means are due to random chance or a significant effect.\n",
        "\n",
        "# How ANOVA Differs from a t-test:\n",
        "# 1. Number of Groups Compared:\n",
        "#    - A t-test is typically used to compare the means of two groups.\n",
        "#    - ANOVA, however, can compare the means of three or more groups in a single test.\n",
        "#    - If ANOVA finds a significant difference, further post hoc tests may identify\n",
        "#      which specific groups differ.\n",
        "\n",
        "# 2. Type I Error Control:\n",
        "#    - Conducting multiple t-tests increases the chance of committing a Type I error\n",
        "#      (incorrectly rejecting a true null hypothesis).\n",
        "#    - ANOVA controls this error by testing all groups at once, reducing the likelihood\n",
        "#      of error accumulation compared to conducting multiple t-tests.\n",
        "\n",
        "# 3. Hypothesis:\n",
        "#    - In a t-test, the null hypothesis assumes that the means of the two groups are equal.\n",
        "#    - In ANOVA, the null hypothesis assumes that all group means are equal, while the\n",
        "#      alternative hypothesis suggests at least one group mean is different.\n",
        "\n",
        "# 4. Output:\n",
        "#    - A t-test provides a t-statistic and p-value for two groups.\n",
        "#    - ANOVA produces an F-statistic and p-value, indicating whether there is a significant\n",
        "#      difference among all group means; further analysis may follow if significant.\n",
        "\n",
        "# Note:\n",
        "#    - While ANOVA tells if there is a difference among groups, it does not specify which\n",
        "#      groups differ. Post hoc tests (e.g., Tukeyâ€™s test) are often used after ANOVA\n",
        "#      to identify specific group differences.\n"
      ],
      "metadata": {
        "id": "ZkQEWdaFm8bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5) Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n",
        "\n",
        "# Ans)\n",
        "# When and Why to Use One-Way ANOVA Instead of Multiple t-tests:\n",
        "\n",
        "# 1. Comparing More Than Two Groups:\n",
        "#    - One-way ANOVA is used when comparing the means of three or more groups.\n",
        "#    - Conducting multiple t-tests for each pair of groups would be inefficient\n",
        "#      and increase the risk of statistical errors.\n",
        "\n",
        "# 2. Reducing Type I Error:\n",
        "#    - Each t-test has its own probability of a Type I error (incorrectly rejecting\n",
        "#      the null hypothesis).\n",
        "#    - Running multiple t-tests increases the cumulative chance of Type I error,\n",
        "#      which is called the family-wise error rate.\n",
        "#    - One-way ANOVA addresses this issue by testing all groups simultaneously,\n",
        "#      keeping the Type I error rate at the desired significance level (e.g., 0.05).\n",
        "\n",
        "# 3. Efficiency and Simplicity:\n",
        "#    - One-way ANOVA provides a single F-statistic and p-value, indicating whether\n",
        "#      there is a statistically significant difference among all group means.\n",
        "#    - This is simpler and more efficient than performing and interpreting multiple\n",
        "#      t-tests for every possible pair of groups.\n",
        "\n",
        "# 4. Identifying Group Differences:\n",
        "#    - One-way ANOVA can detect if there is at least one significant difference among\n",
        "#      groups, without requiring each possible comparison to be tested individually.\n",
        "#    - If ANOVA finds a significant difference, post hoc tests (e.g., Tukey's test)\n",
        "#      can be used to determine specifically which groups differ.\n",
        "\n",
        "# Summary:\n",
        "#    - Use one-way ANOVA when comparing more than two groups to reduce Type I error,\n",
        "#      improve efficiency, and get a clearer understanding of overall group differences.\n",
        "#    - This approach helps in making reliable conclusions without the need for\n",
        "#      multiple tests.\n"
      ],
      "metadata": {
        "id": "7j6QpcwYnIQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6)  Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "# Ans)\n",
        "# Partitioning of Variance in ANOVA:\n",
        "\n",
        "# 1. Total Variance:\n",
        "#    - ANOVA divides the total variance observed in the data into two parts:\n",
        "#      (a) between-group variance and (b) within-group variance.\n",
        "#    - The total variance reflects the overall variation of all data points\n",
        "#      around the grand mean (mean of all groups combined).\n",
        "\n",
        "# 2. Between-Group Variance:\n",
        "#    - This represents the variance due to differences between the means of the groups.\n",
        "#    - It measures how much each group mean deviates from the grand mean.\n",
        "#    - A large between-group variance suggests that there are significant differences\n",
        "#      among the group means.\n",
        "#    - Between-group variance is calculated as the sum of the squared differences\n",
        "#      between each group mean and the grand mean, weighted by the number of observations\n",
        "#      in each group.\n",
        "\n",
        "# 3. Within-Group Variance:\n",
        "#    - This represents the variance within each group, reflecting the natural variability\n",
        "#      or noise within the individual groups.\n",
        "#    - It measures how much individual data points vary around their respective group mean.\n",
        "#    - Within-group variance is calculated as the sum of the squared differences between\n",
        "#      each individual data point and its corresponding group mean.\n",
        "\n",
        "# Contribution to the F-statistic Calculation:\n",
        "\n",
        "# 1. F-statistic Formula:\n",
        "#    - The F-statistic in ANOVA is calculated as the ratio of the between-group variance\n",
        "#      to the within-group variance:\n",
        "#      F = (Between-Group Variance) / (Within-Group Variance)\n",
        "\n",
        "# 2. Interpretation of the F-statistic:\n",
        "#    - A large F-value suggests that the between-group variance is much greater than\n",
        "#      the within-group variance, indicating that at least one group mean is likely\n",
        "#      different from the others.\n",
        "#    - If the F-statistic is close to 1, it suggests that the variances between groups\n",
        "#      are similar to the variances within groups, implying that the group means may\n",
        "#      not be significantly different.\n",
        "\n",
        "# 3. Hypothesis Testing with the F-statistic:\n",
        "#    - The F-statistic is compared against a critical F-value from the F-distribution\n",
        "#      table, based on the degrees of freedom for the between-group and within-group\n",
        "#      variances.\n",
        "#    - If the F-statistic is greater than the critical value, we reject the null hypothesis,\n",
        "#      concluding that there is a significant difference among the group means.\n",
        "\n",
        "# Summary:\n",
        "#    - The partitioning of variance into between-group and within-group components\n",
        "#      helps in isolating the effect of group differences from natural variation.\n",
        "#    - This partitioning forms the basis of the F-test in ANOVA, allowing us to determine\n",
        "#      if observed differences in means are statistically significant.\n"
      ],
      "metadata": {
        "id": "2hyp6-VqsHYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7) Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "# Ans)\n",
        "# Comparison of Classical (Frequentist) and Bayesian Approaches to ANOVA:\n",
        "\n",
        "# 1. Handling Uncertainty:\n",
        "#    - **Classical (Frequentist) ANOVA**: Assumes that parameters (like group means)\n",
        "#      are fixed but unknown quantities. Uncertainty is assessed through p-values,\n",
        "#      which indicate the probability of observing the data (or more extreme values)\n",
        "#      if the null hypothesis is true.\n",
        "#    - **Bayesian ANOVA**: Treats parameters as random variables with probability\n",
        "#      distributions, reflecting the uncertainty about parameter values. This allows\n",
        "#      for a more direct representation of uncertainty through posterior distributions.\n",
        "\n",
        "# 2. Parameter Estimation:\n",
        "#    - **Classical ANOVA**: Estimates parameters like group means and variance using\n",
        "#      point estimates (e.g., sample means and variances) without prior information.\n",
        "#      Confidence intervals provide a range of plausible values for parameters but are\n",
        "#      interpreted differently than Bayesian credible intervals.\n",
        "#    - **Bayesian ANOVA**: Incorporates prior information (beliefs or knowledge about\n",
        "#      parameters before observing the data) to generate posterior distributions.\n",
        "#      Parameters are estimated as probability distributions, allowing for credible\n",
        "#      intervals that show the probability of parameters lying within a certain range.\n",
        "\n",
        "# 3. Hypothesis Testing:\n",
        "#    - **Classical ANOVA**: Uses the F-test to compare variances between groups and\n",
        "#      within groups. It tests a null hypothesis that all group means are equal,\n",
        "#      providing a p-value to indicate the likelihood of the data given the null hypothesis.\n",
        "#    - **Bayesian ANOVA**: Does not rely on p-values. Instead, it calculates the\n",
        "#      posterior probability of different models (e.g., equal or unequal group means).\n",
        "#      Bayesian ANOVA can directly quantify the probability that each hypothesis is true\n",
        "#      based on the data and prior knowledge.\n",
        "\n",
        "# 4. Interpretation of Results:\n",
        "#    - **Classical ANOVA**: Results are interpreted based on p-values and confidence\n",
        "#      intervals. A low p-value (typically < 0.05) indicates that the null hypothesis\n",
        "#      can be rejected, suggesting at least one group mean differs from the others.\n",
        "#    - **Bayesian ANOVA**: Results are interpreted through posterior distributions and\n",
        "#      credible intervals. For example, a 95% credible interval implies there is a 95%\n",
        "#      probability that the true parameter value lies within that interval, which is\n",
        "#      more intuitive than confidence intervals in frequentist statistics.\n",
        "\n",
        "# 5. Flexibility and Assumptions:\n",
        "#    - **Classical ANOVA**: Generally assumes equal variances across groups and requires\n",
        "#      normally distributed data, which limits its flexibility.\n",
        "#    - **Bayesian ANOVA**: Offers greater flexibility, as it can incorporate prior\n",
        "#      distributions that accommodate unequal variances and handle data that deviate\n",
        "#      from normality more easily.\n",
        "\n",
        "# Summary:\n",
        "#    - The classical approach to ANOVA provides objective, p-value-based conclusions\n",
        "#      for hypothesis testing but lacks flexibility in handling prior information.\n",
        "#    - The Bayesian approach, on the other hand, provides a more intuitive representation\n",
        "#      of uncertainty, allows for prior knowledge integration, and offers posterior\n",
        "#      probabilities for hypotheses, making it useful when there is prior knowledge\n",
        "#      or when more flexible assumptions are needed.\n"
      ],
      "metadata": {
        "id": "2U9tC1U8sRPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8) Question: You have two sets of data representing the incomes of two different professions\n",
        " #Profession A: [48, 52, 55, 60, 62\n",
        " #Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "#incomes are equal. What are your conclusions based on the F-test?\n",
        " #Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        " #Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison\n",
        "\n",
        " # Ans)\n",
        " # Importing the required libraries\n",
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Income data for two professions\n",
        "profession_A = [48, 52, 55, 60, 62]\n",
        "profession_B = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Step 1: Calculate variances for both samples\n",
        "variance_A = np.var(profession_A, ddof=1)  # Sample variance for Profession A\n",
        "variance_B = np.var(profession_B, ddof=1)  # Sample variance for Profession B\n",
        "\n",
        "# Step 2: Calculate the F-statistic\n",
        "# F-statistic is the ratio of the larger variance to the smaller variance\n",
        "F_statistic = variance_A / variance_B if variance_A > variance_B else variance_B / variance_A\n",
        "\n",
        "# Step 3: Calculate degrees of freedom for each group\n",
        "df_A = len(profession_A) - 1  # Degrees of freedom for Profession A\n",
        "df_B = len(profession_B) - 1  # Degrees of freedom for Profession B\n",
        "\n",
        "# Step 4: Calculate the p-value\n",
        "# Since this is a two-tailed test, we consider both ends of the distribution.\n",
        "p_value = 2 * min(f.cdf(F_statistic, df_A, df_B), 1 - f.cdf(F_statistic, df_A, df_B))\n",
        "\n",
        "# Step 5: Output the results\n",
        "print(\"F-statistic:\", F_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpretation:\n",
        "# If the p-value is less than the significance level (e.g., 0.05), we reject the null hypothesis,\n",
        "# indicating that the variances of the two professions are significantly different.\n",
        "# Otherwise, we do not reject the null hypothesis, suggesting no significant difference\n",
        "# between the variances of incomes for Profession A and Profession B.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pYWfcbdsgt7",
        "outputId": "e7db002c-3e6c-449e-ab6a-64ffe90a21d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.49304859900533904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9) Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in #average heights between three different regions with the following data\n",
        " #Region A: [160, 162, 165, 158, 164\n",
        " #Region B: [172, 175, 170, 168, 174\n",
        " #Region C: [180, 182, 179, 185, 183\n",
        " #Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        " #Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value\n",
        "\n",
        "#Ans)\n",
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Height data for three different regions\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Step 1: Conduct one-way ANOVA\n",
        "F_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "# Step 2: Output the results\n",
        "print(\"F-statistic:\", F_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpretation:\n",
        "# If the p-value is less than the significance level (e.g., 0.05), we reject the null hypothesis,\n",
        "# indicating that there are statistically significant differences in average heights among the regions.\n",
        "# Otherwise, we do not reject the null hypothesis, suggesting no significant differences in average heights.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXO2VHP0s71c",
        "outputId": "e28eb5a3-2bc3-45ca-f45a-65175abfb90e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n"
          ]
        }
      ]
    }
  ]
}